{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Resizing, Rescaling, Dense, Conv2D, MaxPooling2D, Flatten, RandomFlip, RandomContrast, RandomRotation\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.image import random_flip_up_down, random_flip_left_right, random_contrast, random_brightness, rot90\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproduceability\n",
    "SEED = 42\n",
    "\n",
    "# Size to convert images to (pixels)\n",
    "RESIZE_HEIGHT = 750\n",
    "RESIZE_WIDTH = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 files belonging to 2 classes.\n",
      "Found 1311 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load photo data\n",
    "train_images = image_dataset_from_directory(directory = 'data/Training', color_mode=\"grayscale\", label_mode = 'binary', batch_size=32)\n",
    "test_images = image_dataset_from_directory(directory = 'data/Testing', color_mode=\"grayscale\", label_mode = 'binary', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images are not standardized. Need to standardize prior to modelling\n",
    "def resize_images(image, label):\n",
    "    image = tf.image.resize_with_crop_or_pad(image, target_height = RESIZE_HEIGHT, target_width = RESIZE_WIDTH)\n",
    "    return image, label\n",
    "\n",
    "train_images = train_images.map(resize_images)\n",
    "test_images = test_images.map(resize_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime optimization\n",
    "AUTOTUNE = tf.data.AUTOTUNE \n",
    "train_images = train_images.prefetch(buffer_size = AUTOTUNE)\n",
    "test_images = test_images.prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial photo standardization preprocessing segment.\n",
    "# Input shape = (height, width, 1 = greyscale)\n",
    "standardization_layer = Sequential([\n",
    "    #Resizing(RESIZE_WIDTH, RESIZE_HEIGHT),\n",
    "    Rescaling(1 / 255.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 750, 750, 1) (32, 1)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_images.take(1):\n",
    "    print(images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an augmentation layer that randomly rotates, flips, and contrasts the images.\n",
    "augmentation_layer = Sequential([\n",
    "    tf.keras.Input(shape = (RESIZE_HEIGHT, RESIZE_WIDTH, 1)),\n",
    "    RandomRotation(factor = (-0.3, 0.3)),\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomContrast(factor = (-0.3, 0.3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model with combination of preprocessing layers and convolutional/pooling/dense layers\n",
    "model = Sequential([\n",
    "    standardization_layer,\n",
    "    augmentation_layer,\n",
    "    Conv2D(16, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9589 - loss: 0.1266 - recall: 0.9746 - val_accuracy: 0.9344 - val_loss: 0.1788 - val_recall: 0.9614\n",
      "Epoch 2/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9644 - loss: 0.1104 - recall: 0.9790 - val_accuracy: 0.9375 - val_loss: 0.1634 - val_recall: 0.9349\n",
      "Epoch 3/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9623 - loss: 0.1136 - recall: 0.9755 - val_accuracy: 0.9542 - val_loss: 0.1587 - val_recall: 0.9558\n",
      "Epoch 4/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9653 - loss: 0.1064 - recall: 0.9804 - val_accuracy: 0.9252 - val_loss: 0.2006 - val_recall: 0.9625\n",
      "Epoch 5/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 1s/step - accuracy: 0.9679 - loss: 0.0995 - recall: 0.9826 - val_accuracy: 0.9428 - val_loss: 0.1608 - val_recall: 0.9558\n",
      "Epoch 6/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9702 - loss: 0.0966 - recall: 0.9813 - val_accuracy: 0.9405 - val_loss: 0.1485 - val_recall: 0.9581\n",
      "Epoch 7/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 1s/step - accuracy: 0.9693 - loss: 0.0893 - recall: 0.9812 - val_accuracy: 0.9489 - val_loss: 0.1327 - val_recall: 0.9603\n",
      "Epoch 8/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9698 - loss: 0.0924 - recall: 0.9812 - val_accuracy: 0.9138 - val_loss: 0.2373 - val_recall: 0.9868\n",
      "Epoch 9/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 1s/step - accuracy: 0.9715 - loss: 0.0887 - recall: 0.9848 - val_accuracy: 0.9596 - val_loss: 0.1085 - val_recall: 0.9669\n",
      "Epoch 10/10\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 1s/step - accuracy: 0.9808 - loss: 0.0642 - recall: 0.9896 - val_accuracy: 0.9428 - val_loss: 0.1633 - val_recall: 0.9316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16c09acc0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy','recall'])\n",
    "\n",
    "model.fit(train_images, validation_data=test_images, epochs=10, batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
