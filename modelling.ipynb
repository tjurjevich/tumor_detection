{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Resizing, Rescaling, Dense, Conv2D, MaxPooling2D, Flatten, RandomFlip, RandomContrast, RandomRotation, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.image import random_flip_up_down, random_flip_left_right, random_contrast, random_brightness, rot90\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproduceability\n",
    "SEED = 42\n",
    "\n",
    "# Size to convert images to (pixels)\n",
    "RESIZE_HEIGHT = 750\n",
    "RESIZE_WIDTH = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 files belonging to 2 classes.\n",
      "Found 1311 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load photo data\n",
    "train_images = image_dataset_from_directory(directory = 'data/Training', color_mode=\"grayscale\", label_mode = 'binary', batch_size=32)\n",
    "test_images = image_dataset_from_directory(directory = 'data/Testing', color_mode=\"grayscale\", label_mode = 'binary', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images are not standardized. Need to standardize prior to modelling\n",
    "def resize_images(image, label):\n",
    "    image = tf.image.resize_with_crop_or_pad(image, target_height = RESIZE_HEIGHT, target_width = RESIZE_WIDTH)\n",
    "    return image, label\n",
    "\n",
    "train_images = train_images.map(resize_images)\n",
    "test_images = test_images.map(resize_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime optimization\n",
    "AUTOTUNE = tf.data.AUTOTUNE \n",
    "train_images = train_images.prefetch(buffer_size = AUTOTUNE)\n",
    "test_images = test_images.prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom classifier method\n",
    "'''\n",
    "Inputs -> Preprocessing -> Augmentation (training) -> Convolution ->\n",
    "Max Pooling -> Convolution -> Max Pooling -> Flatten -> Dense ->\n",
    "Dense -> Dropout -> Dense (classifier)\n",
    "'''\n",
    "class v1TumorClassifier(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Standardization layer that resizes to and scales  \n",
    "        self.standardization_layer = Sequential([\n",
    "            Rescaling(1 / 255.)\n",
    "        ])\n",
    "        self.augmentation_layer = Sequential([\n",
    "            tf.keras.Input(shape = (RESIZE_HEIGHT, RESIZE_WIDTH, 1)),\n",
    "            RandomRotation(factor = (-0.3, 0.3)),\n",
    "            RandomFlip(\"horizontal_and_vertical\"),\n",
    "            RandomContrast(factor = (-0.3, 0.3))\n",
    "        ])\n",
    "\n",
    "        self.conv1 = Conv2D(16, (3,3), activation = 'relu')\n",
    "        self.pool1 = MaxPooling2D()\n",
    "        self.conv2 = Conv2D(32, (3,3), activation = 'relu')\n",
    "        self.pool2 = MaxPooling2D()\n",
    "\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.dense1 = Dense(128, activation = 'relu')\n",
    "        self.dense2 = Dense(64, activation = 'relu')\n",
    "\n",
    "        self.classifier = Dense(1, activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        x = self.standardization_layer(inputs)\n",
    "        x = self.augmentation_layer(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "        output = self.classifier(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 1s/step - accuracy: 0.7556 - loss: 0.5685 - recall: 0.8758 - val_accuracy: 0.8162 - val_loss: 0.3967 - val_recall: 0.9724\n",
      "Epoch 2/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 1s/step - accuracy: 0.9183 - loss: 0.2386 - recall: 0.9634 - val_accuracy: 0.8436 - val_loss: 0.3664 - val_recall: 0.9570\n",
      "Epoch 3/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 1s/step - accuracy: 0.9412 - loss: 0.1726 - recall: 0.9674 - val_accuracy: 0.8612 - val_loss: 0.3715 - val_recall: 0.9547\n",
      "Epoch 4/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 1s/step - accuracy: 0.9514 - loss: 0.1528 - recall: 0.9731 - val_accuracy: 0.9069 - val_loss: 0.2438 - val_recall: 0.9172\n",
      "Epoch 5/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 1s/step - accuracy: 0.9553 - loss: 0.1366 - recall: 0.9684 - val_accuracy: 0.9108 - val_loss: 0.2222 - val_recall: 0.9470\n"
     ]
    }
   ],
   "source": [
    "custom_model = v1TumorClassifier()\n",
    "\n",
    "custom_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy','recall']  \n",
    ")\n",
    "\n",
    "custom_model_history = custom_model.fit(train_images, validation_data = test_images, epochs = 5, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5712 files belonging to 2 classes.\n",
      "Found 1311 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Transfer learning method(s): ResNet and DenseNet\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
    "\n",
    "V2_IMAGE_SIZE = (256,256)\n",
    "\n",
    "# Load photo data\n",
    "v2_train_images = image_dataset_from_directory(directory = 'data/Training', color_mode=\"rgb\", label_mode = 'binary', batch_size=32)\n",
    "v2_test_images = image_dataset_from_directory(directory = 'data/Testing', color_mode=\"rgb\", label_mode = 'binary', batch_size=32)\n",
    "\n",
    "def v2_resize_images(image, label):\n",
    "    image = tf.image.resize(image, method='bilinear', size = V2_IMAGE_SIZE)\n",
    "    return image, label\n",
    "\n",
    "train_images = train_images.map(v2_resize_images)\n",
    "test_images = test_images.map(v2_resize_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning classifier method\n",
    "'''\n",
    "Inputs -> Preprocessing -> Augmentation (training) -> Pretrained Base ->\n",
    "Global Max/Avg Pooling -> Dense -> Dropout -> Dense (classifier)\n",
    "'''\n",
    "class v2TumorClassifier(tf.keras.Model):\n",
    "    def __init__(self, base_model = Literal['resnet','densenet'], pool_type = Literal['max','avg']):\n",
    "        super().__init__()\n",
    "\n",
    "        # Base model assignment\n",
    "        if base_model == 'resnet':\n",
    "            self.base_model = ResNet50(\n",
    "                weights = 'imagenet',\n",
    "                include_top = False,\n",
    "                input_shape = (V2_IMAGE_SIZE[0], V2_IMAGE_SIZE[1], 3)\n",
    "            )\n",
    "\n",
    "        if base_model == 'densenet':\n",
    "            self.base_model = DenseNet121(\n",
    "                weights = 'imagenet',\n",
    "                include_top = False,\n",
    "                input_shape = (V2_IMAGE_SIZE[0], V2_IMAGE_SIZE[1], 3)\n",
    "            )\n",
    "\n",
    "        # Base model should not be getting retrained\n",
    "        self.base_model.trainable = False\n",
    "\n",
    "        # Standardization layer that resizes to and scales  \n",
    "        self.standardization_layer = Sequential([\n",
    "            Rescaling(1 / 255.)\n",
    "        ])\n",
    "        # Augmentation layer that augments data\n",
    "        self.augmentation_layer = Sequential([\n",
    "            tf.keras.Input(shape = (V2_IMAGE_SIZE[0], V2_IMAGE_SIZE[1], 3)),\n",
    "            RandomRotation(factor = (-0.3, 0.3)),\n",
    "            RandomFlip(\"horizontal_and_vertical\"),\n",
    "            RandomContrast(factor = (-0.3, 0.3))\n",
    "        ])\n",
    "\n",
    "        # Global pooling layer\n",
    "        if pool_type == 'max':\n",
    "            self.pool_layer = GlobalMaxPooling2D()\n",
    "        if pool_type == 'avg':\n",
    "            self.pool_layer = GlobalAveragePooling2D()\n",
    "\n",
    "        # Dropout, dense, classifier\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.dense = Dense(32, activation = 'relu')\n",
    "        self.classifier = Dense(1, activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training = False):\n",
    "        x = self.standardization_layer(inputs)\n",
    "        x = self.augmentation_layer(x)\n",
    "        x = self.base_model(x, training = training)\n",
    "        x = self.pool_layer(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 551ms/step - accuracy: 0.8452 - loss: 0.4053 - recall: 0.9551 - val_accuracy: 0.7452 - val_loss: 0.5945 - val_recall: 0.9901\n",
      "Epoch 2/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 548ms/step - accuracy: 0.8999 - loss: 0.3005 - recall: 0.9793 - val_accuracy: 0.7872 - val_loss: 0.4732 - val_recall: 0.9757\n",
      "Epoch 3/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 541ms/step - accuracy: 0.9056 - loss: 0.2930 - recall: 0.9792 - val_accuracy: 0.7628 - val_loss: 0.5361 - val_recall: 0.9868\n",
      "Epoch 4/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 538ms/step - accuracy: 0.9049 - loss: 0.2769 - recall: 0.9800 - val_accuracy: 0.7536 - val_loss: 0.5436 - val_recall: 0.9857\n",
      "Epoch 5/5\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 544ms/step - accuracy: 0.9079 - loss: 0.2660 - recall: 0.9789 - val_accuracy: 0.7879 - val_loss: 0.4734 - val_recall: 0.9746\n"
     ]
    }
   ],
   "source": [
    "resnet_1 = v2TumorClassifier(base_model = 'resnet', pool_type = 'avg')\n",
    "\n",
    "resnet_1.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy','recall']\n",
    ")\n",
    "\n",
    "resnet_1_history = resnet_1.fit(v2_train_images, validation_data = v2_test_images, epochs = 5, batch_size = 256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
